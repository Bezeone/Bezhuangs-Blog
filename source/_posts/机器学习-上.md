---
title: 机器学习（上）
date: 2021-09-23
updated: 2021-09-25
tags: [Machine Learning]
group: todo
mathjax: true
categories: 计算机专业
references:
  - title: 李宏毅机器学习2017笔记
    url: https://datawhalechina.github.io/leeml-notes/#/
---

> 机器学习是人工智能的重要技术基础，涉及的内容十分广泛，涵盖概率论知识，统计学知识，近似理论知识和复杂算法知识，使用计算机作为工具并致力于真实实时的模拟人类学习方式。我选择的课程是台湾大学电机工程系助理教授李宏毅的 [ML 2021 Spring](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html)，这门课程从最基本的观念讲到最前瞻的技术，重点讲解深度学习，同时新增了 Transformer、流模型 Glow 和对抗攻击等最新技术内容。以下为所记课堂笔记第一部分，包含机器学习基本概念理论、深度学习和自注意力机制，可供参考。其他部分笔记详见[机器学习（中）](/机器学习-中)和[机器学习（下）](/机器学习-下)

<!--more-->

{% image https://cdn.jsdelivr.net/gh/Bezhuang/Imgbed/blogimg/李宏毅机器学习01.jpg, width=500px %}

### 机器学习介绍

#### 机器学习基本概念

- Machine Learning ≈ Looking for Function（寻找复杂函数的过程）
- 机器学习的分类
  - Regression（回归）: The function outputs a scalar
  - Classification（分类）: Given options (classes), the function outputs the correct one
  - Structured Learning（结构化学习）: create something with structure (image, document)

- Regression（回归）的 Training 过程

  1. Function with Unknown Parameters, which is based on domain knowledge

     - Model: $y=b+w x_1$
     - $w$ (weight) and $b$ (bias) are unknown parameters learned from data（模型参数）

  2. Define Loss from Training Data

     - Loss is a function of parameters: $L(b,w)$
     - Loss 用来评估未知参数准确度 (how good a set of values is): $L=\dfrac{1}{N}\sum _{n}e_{n}$
     - MAE（绝对平均误差）: 𝐿 is mean absolute error: $e=\left|y-\widehat{y}\right|$
     - MSE（均方误差）: 𝐿 is mean square error: $e=\left( y-\widehat{y}\right)^{2}$
     - If $𝑦$ and $\widehat{y}$ are both probability distributions（当预测值和真实值是概率分布时）：选择交叉熵（Cross-entropy）作为 Loss 函数

  3. Optimization（优化）: $w^{\ast },b^{\ast }=\arg \min _{w,b}L$

     - 最常用的方法：梯度下降（Gradient Descent）
     - 首先随机初始化参数：(Randomly) Pick initial values $w^0,b^0$,
     - 再计算该点的 Loss 关于参数的微分值：Compute $\dfrac{\partial L}{\partial w}| w=w^{0},b=b^{0}$
     - 当微分为负数时，Loss 关于 w 递减，增大 w 使 Loss下降。当微分为正数时，Loss 关于 w 递增，减小 w 使 Loss 下降
     - 随之用微分值去更新参数：Update $𝑤,b$ iteratively 
     - 跨越的大小通过学习率控制，方向通过符号实现：$w_{0}\rightarrow w_{1},b_{0}\rightarrow b_{1}$: $\eta \dfrac{2L}{\partial w}| w=w^{0},b=b^{0}$
     - 学习率（Learning rate）是一种超参数（hyperparameter），记做：$\eta$

- Linear models are too simple and have severe limitations (model bias), we need more sophisticated and flexible models

#### 深度学习基本概念

- 足够多的分段线性曲线可以近似于连续曲线（Approximate continuous curve by a piecewise linear curve, to have good approximation, we need sufficient pieces），使模型更有弹性
  - 分段线性曲线（Piecewise linear curve）：a set of hard sigmoid 函数加上常数组成，分段线性曲线的折线越多，需要的 Hard sigmoid 函数就更多
  - Hard sigmoid 不能作为基础函数，因为折线的转角处无法求微分，所以要用一个平滑的曲线做为基础函数，也就是 Sigmoid（S型曲线）
  - 使用不同的 $w$、$b$、$c$ 参数可以实现各种各样的 Sigmoid 函数，$w$ 控制 slopes ，$b$ 控制左右 shifts，$c$ 控制 height：$y=c\cdot \dfrac{1}{1+e^{-\left( b+wx_{1}\right) }}=c\cdot signoid\left( b+wx_{1}\right)$
  - Linear model 由 $y=b+wx_{1}$ 变为更有弹性的：$y=b+\sum _{i}c_{i}\cdot sigmoid\left( b_{i}+w_{i}x_{1}\right)$

- 当模型有多个 Features（自变量）时：$y=b+\sum _{i}c_{i}\cdot sigmoid\left( b_{i}+\sum _{j}w_{ij}x_{j}\right)$
- 


### 深度学习

#### 机器学习任务攻略

#### 类神经网络训练不起来怎么办（一）？局部最小值与鞍点

#### 类神经网络训练不起来怎么办（二）？批次与动量

#### 类神经网络训练不起来怎么办（三）？自动调整学习速率

#### 类神经网络训练不起来怎么办（四）？损失函数也可能有影响



### 自注意力

#### 卷积神经网络

#### 自注意力机制（上）

#### 自注意力机制（下）



### 机器学习理论

#### Theory of ML

