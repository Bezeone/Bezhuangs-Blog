---
title: æœºå™¨å­¦ä¹ ï¼ˆä¸Šï¼‰
date: 2021-09-23
updated: 2021-09-25
tags: [Machine Learning]
group: todo
mathjax: true
categories: è®¡ç®—æœºä¸“ä¸š
references:
  - title: æå®æ¯…æœºå™¨å­¦ä¹ 2017ç¬”è®°
    url: https://datawhalechina.github.io/leeml-notes/#/
---

> æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦æŠ€æœ¯åŸºç¡€ï¼Œæ¶‰åŠçš„å†…å®¹ååˆ†å¹¿æ³›ï¼Œæ¶µç›–æ¦‚ç‡è®ºçŸ¥è¯†ï¼Œç»Ÿè®¡å­¦çŸ¥è¯†ï¼Œè¿‘ä¼¼ç†è®ºçŸ¥è¯†å’Œå¤æ‚ç®—æ³•çŸ¥è¯†ï¼Œä½¿ç”¨è®¡ç®—æœºä½œä¸ºå·¥å…·å¹¶è‡´åŠ›äºçœŸå®å®æ—¶çš„æ¨¡æ‹Ÿäººç±»å­¦ä¹ æ–¹å¼ã€‚æˆ‘é€‰æ‹©çš„è¯¾ç¨‹æ˜¯å°æ¹¾å¤§å­¦ç”µæœºå·¥ç¨‹ç³»åŠ©ç†æ•™æˆæå®æ¯…çš„ [ML 2021 Spring](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html)ï¼Œè¿™é—¨è¯¾ç¨‹ä»æœ€åŸºæœ¬çš„è§‚å¿µè®²åˆ°æœ€å‰ç»çš„æŠ€æœ¯ï¼Œé‡ç‚¹è®²è§£æ·±åº¦å­¦ä¹ ï¼ŒåŒæ—¶æ–°å¢äº† Transformerã€æµæ¨¡å‹ Glow å’Œå¯¹æŠ—æ”»å‡»ç­‰æœ€æ–°æŠ€æœ¯å†…å®¹ã€‚ä»¥ä¸‹ä¸ºæ‰€è®°è¯¾å ‚ç¬”è®°ç¬¬ä¸€éƒ¨åˆ†ï¼ŒåŒ…å«æœºå™¨å­¦ä¹ åŸºæœ¬æ¦‚å¿µç†è®ºã€æ·±åº¦å­¦ä¹ å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä¾›å‚è€ƒã€‚å…¶ä»–éƒ¨åˆ†ç¬”è®°è¯¦è§[æœºå™¨å­¦ä¹ ï¼ˆä¸­ï¼‰](/æœºå™¨å­¦ä¹ -ä¸­)å’Œ[æœºå™¨å­¦ä¹ ï¼ˆä¸‹ï¼‰](/æœºå™¨å­¦ä¹ -ä¸‹)

<!--more-->

{% image https://cdn.jsdelivr.net/gh/Bezhuang/Imgbed/blogimg/æå®æ¯…æœºå™¨å­¦ä¹ 01.jpg, width=500px %}

### æœºå™¨å­¦ä¹ ä»‹ç»

#### æœºå™¨å­¦ä¹ åŸºæœ¬æ¦‚å¿µ

- Machine Learning â‰ˆ Looking for Functionï¼ˆå¯»æ‰¾å¤æ‚å‡½æ•°çš„è¿‡ç¨‹ï¼‰
- æœºå™¨å­¦ä¹ çš„åˆ†ç±»
  - Regressionï¼ˆå›å½’ï¼‰: The function outputs a scalar
  - Classificationï¼ˆåˆ†ç±»ï¼‰: Given options (classes), the function outputs the correct one
  - Structured Learningï¼ˆç»“æ„åŒ–å­¦ä¹ ï¼‰: create something with structure (image, document)

- Regressionï¼ˆå›å½’ï¼‰çš„ Training è¿‡ç¨‹

  1. Function with Unknown Parameters, which is based on domain knowledge

     - Model: $y=b+w x_1$
     - $w$ (weight) and $b$ (bias) are unknown parameters learned from dataï¼ˆæ¨¡å‹å‚æ•°ï¼‰

  2. Define Loss from Training Data

     - Loss is a function of parameters: $L(b,w)$
     - Loss ç”¨æ¥è¯„ä¼°æœªçŸ¥å‚æ•°å‡†ç¡®åº¦ (how good a set of values is): $L=\dfrac{1}{N}\sum _{n}e_{n}$
     - MAEï¼ˆç»å¯¹å¹³å‡è¯¯å·®ï¼‰: ğ¿ is mean absolute error: $e=\left|y-\widehat{y}\right|$
     - MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰: ğ¿ is mean square error: $e=\left( y-\widehat{y}\right)^{2}$
     - If $ğ‘¦$ and $\widehat{y}$ are both probability distributionsï¼ˆå½“é¢„æµ‹å€¼å’ŒçœŸå®å€¼æ˜¯æ¦‚ç‡åˆ†å¸ƒæ—¶ï¼‰ï¼šé€‰æ‹©äº¤å‰ç†µï¼ˆCross-entropyï¼‰ä½œä¸º Loss å‡½æ•°

  3. Optimizationï¼ˆä¼˜åŒ–ï¼‰: $w^{\ast },b^{\ast }=\arg \min _{w,b}L$

     - æœ€å¸¸ç”¨çš„æ–¹æ³•ï¼šæ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰
     - é¦–å…ˆéšæœºåˆå§‹åŒ–å‚æ•°ï¼š(Randomly) Pick initial values $w^0,b^0$,
     - å†è®¡ç®—è¯¥ç‚¹çš„ Loss å…³äºå‚æ•°çš„å¾®åˆ†å€¼ï¼šCompute $\dfrac{\partial L}{\partial w}| w=w^{0},b=b^{0}$
     - å½“å¾®åˆ†ä¸ºè´Ÿæ•°æ—¶ï¼ŒLoss å…³äº w é€’å‡ï¼Œå¢å¤§ w ä½¿ Lossä¸‹é™ã€‚å½“å¾®åˆ†ä¸ºæ­£æ•°æ—¶ï¼ŒLoss å…³äº w é€’å¢ï¼Œå‡å° w ä½¿ Loss ä¸‹é™
     - éšä¹‹ç”¨å¾®åˆ†å€¼å»æ›´æ–°å‚æ•°ï¼šUpdate $ğ‘¤,b$ iteratively 
     - è·¨è¶Šçš„å¤§å°é€šè¿‡å­¦ä¹ ç‡æ§åˆ¶ï¼Œæ–¹å‘é€šè¿‡ç¬¦å·å®ç°ï¼š$w_{0}\rightarrow w_{1},b_{0}\rightarrow b_{1}$: $\eta \dfrac{2L}{\partial w}| w=w^{0},b=b^{0}$
     - å­¦ä¹ ç‡ï¼ˆLearning rateï¼‰æ˜¯ä¸€ç§è¶…å‚æ•°ï¼ˆhyperparameterï¼‰ï¼Œè®°åšï¼š$\eta$

- Linear models are too simple and have severe limitations (model bias), we need more sophisticated and flexible models

#### æ·±åº¦å­¦ä¹ åŸºæœ¬æ¦‚å¿µ

- è¶³å¤Ÿå¤šçš„åˆ†æ®µçº¿æ€§æ›²çº¿å¯ä»¥è¿‘ä¼¼äºè¿ç»­æ›²çº¿ï¼ˆApproximate continuous curve by a piecewise linear curve, to have good approximation, we need sufficient piecesï¼‰ï¼Œä½¿æ¨¡å‹æ›´æœ‰å¼¹æ€§
  - åˆ†æ®µçº¿æ€§æ›²çº¿ï¼ˆPiecewise linear curveï¼‰ï¼ša set of hard sigmoid å‡½æ•°åŠ ä¸Šå¸¸æ•°ç»„æˆï¼Œåˆ†æ®µçº¿æ€§æ›²çº¿çš„æŠ˜çº¿è¶Šå¤šï¼Œéœ€è¦çš„ Hard sigmoid å‡½æ•°å°±æ›´å¤š
  - Hard sigmoid ä¸èƒ½ä½œä¸ºåŸºç¡€å‡½æ•°ï¼Œå› ä¸ºæŠ˜çº¿çš„è½¬è§’å¤„æ— æ³•æ±‚å¾®åˆ†ï¼Œæ‰€ä»¥è¦ç”¨ä¸€ä¸ªå¹³æ»‘çš„æ›²çº¿åšä¸ºåŸºç¡€å‡½æ•°ï¼Œä¹Ÿå°±æ˜¯ Sigmoidï¼ˆSå‹æ›²çº¿ï¼‰
  - ä½¿ç”¨ä¸åŒçš„ $w$ã€$b$ã€$c$ å‚æ•°å¯ä»¥å®ç°å„ç§å„æ ·çš„ Sigmoid å‡½æ•°ï¼Œ$w$ æ§åˆ¶ slopes ï¼Œ$b$ æ§åˆ¶å·¦å³ shiftsï¼Œ$c$ æ§åˆ¶ heightï¼š$y=c\cdot \dfrac{1}{1+e^{-\left( b+wx_{1}\right) }}=c\cdot signoid\left( b+wx_{1}\right)$
  - Linear model ç”± $y=b+wx_{1}$ å˜ä¸ºæ›´æœ‰å¼¹æ€§çš„ï¼š$y=b+\sum _{i}c_{i}\cdot sigmoid\left( b_{i}+w_{i}x_{1}\right)$

- å½“æ¨¡å‹æœ‰å¤šä¸ª Featuresï¼ˆè‡ªå˜é‡ï¼‰æ—¶ï¼š$y=b+\sum _{i}c_{i}\cdot sigmoid\left( b_{i}+\sum _{j}w_{ij}x_{j}\right)$
- 


### æ·±åº¦å­¦ä¹ 

#### æœºå™¨å­¦ä¹ ä»»åŠ¡æ”»ç•¥

#### ç±»ç¥ç»ç½‘ç»œè®­ç»ƒä¸èµ·æ¥æ€ä¹ˆåŠï¼ˆä¸€ï¼‰ï¼Ÿå±€éƒ¨æœ€å°å€¼ä¸éç‚¹

#### ç±»ç¥ç»ç½‘ç»œè®­ç»ƒä¸èµ·æ¥æ€ä¹ˆåŠï¼ˆäºŒï¼‰ï¼Ÿæ‰¹æ¬¡ä¸åŠ¨é‡

#### ç±»ç¥ç»ç½‘ç»œè®­ç»ƒä¸èµ·æ¥æ€ä¹ˆåŠï¼ˆä¸‰ï¼‰ï¼Ÿè‡ªåŠ¨è°ƒæ•´å­¦ä¹ é€Ÿç‡

#### ç±»ç¥ç»ç½‘ç»œè®­ç»ƒä¸èµ·æ¥æ€ä¹ˆåŠï¼ˆå››ï¼‰ï¼ŸæŸå¤±å‡½æ•°ä¹Ÿå¯èƒ½æœ‰å½±å“



### è‡ªæ³¨æ„åŠ›

#### å·ç§¯ç¥ç»ç½‘ç»œ

#### è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¸Šï¼‰

#### è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¸‹ï¼‰



### æœºå™¨å­¦ä¹ ç†è®º

#### Theory of ML

