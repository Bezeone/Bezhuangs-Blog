---
title: 白话机器学习算法
date: 2021-10-31
tags: []
categories: 读书笔记
---

> 机器学习算法是数据科学的力量之源，它和数据一起产生极其宝贵的知识并且帮助我们以新的方式利用已有信息。[白话机器学习算法](https://book.douban.com/subject/30442187/)一书用通俗易懂的语言以及大量有趣的示例和插图讲解10多种前沿的机器学习算法。内容涵盖k均值聚类、主成分分析、关联规则、社会网络分析等无监督学习算法，以及回归分析、k最近邻、支持向量机、决策树、随机森林、神经网络等监督学习算法，并概述强化学习算法的思想。以下为阅读过程中所作的读书笔记，可供参考。

<!--more-->

### 为何需要数据科学

- 借助现代计算机和高级算法，我们能够做到以下几点：
  1. 从大型数据集中发现隐藏的趋势
  2.  充分利用发现的趋势做预测
  3. 计算每种结果出现的概率
  4. 快速获取准确结果

### 基础知识

#### 准备数据

- 数据格式：数据点、数据集
- 变量（variable）：用于描述数据点，又叫属性、特征或维度
  - 二值变量（binary variable）：最简单的变量类型，只有2个可选值
  - 分类变量（categorical variable）：用来表示有2个以上选择的情况
  - 整型变量（integer variable）：用来表示整数
  - 连续变量（continuous variable）：用来表示小数
- 变量选择是一个试错的过程，需要根据反馈结果不断更换变量，选取那些最有希望的变量，以待进一步分析
- 特征工程（feature engineering）是将原始数据转化成更好的表达问题本质的特征的过程，使得将这些特征运用到预测模型中能提高对不可见数据的模型预测精度
- 解决数据缺失：近似、计算、移除

#### 选择算法

- 无监督学习（unsupervised learning）：依靠算法从数据中找出隐藏的模式
  - k 均值聚类、主成分分析、关联规则、社会网络分析
  - 通过间接手段，可以对无监督学习模型输出的结果进行验证
- 监督学习（supervised learning）：基于数据中已有的模式做预测
  - 回归分析、k 最近邻、支持向量机、决策树、随机森林（random forest）、神经网络
- 强化学习（reinforcement learning）：使用数据中的模式做预测，并根据越来越多的反馈结果不断改进
- 除了要了解算法使用的任务类型外，还要了解各种算法对不同数据类型的分析能力，以及结果的本质

#### 参数调优（parameter tuning）

- 不同的算法有不同的调节参数，即便是同一个算法，如果参数调的不一样，所产生的结果也各不相同
- 过拟合模型：过度敏感，把数据中的随机波动当成持久模式，对当前数据有着很高的预测准确度，但是对未知数据的预测准确度较差（泛化能力不强）
- 欠拟合模型（underfitting）：过于愚钝，很可能会忽视数据中的重要趋势，这会导致模型对当前数据和未知数据的预测准确度下降
- 理想拟合：算法能在识别主要趋势和忽视微小变化之间找到平衡，使最终得到的模型非常适合做预测
- 对于大多数模型而言，过拟合（overfitting）是常见问题，所以增加预测模型的复杂度能最大限度的减少预测误差，但容易出现预测边界过度复杂
- 通过正则化（regularization）引入惩罚参数，通过人为增大预测误差，对模型复杂度的增加进行惩罚，从而使算法同时考虑复杂度和准确度使模型保持简单，有助于提高模型的泛化能力

#### 评价模型

- 使用一些评价指标来比较模型的预测准确度：预测准确率、混淆矩阵和均方根误差（root mean squared error）
- 分类指标（classification）
  - 预测准确率：正确预测所占的比率，无法通过预测准确率得知预测误差是如何产生的
  - 混淆矩阵（confusion matrix）：在预测准确率的基础上添加假正类型和假负类型，辨别预测误差数
- 回归指标
  - 由于回归预测使用连续值，因此误差一般被量化成预测值和实际值之差，惩罚随误差大小而不同
  - 均方根误差：将每个误差都取平方，放大大误差，这使得均方根误差对异常值极其敏感，对这些值的惩罚力度也更大，避免较大的误差
- 验证：评估模型对新数据的预测准确度，避免过拟合模型在面对当前数据表现良好而面对新数据时可能表现糟糕的情况
- 在评估模型时并不一定非要使用新数据，而是可以把当前的数据集划分成训练集（training dataset）和测试集（test dataset）
  - 训练集用来生成和调整预测模型，测试集用来充当新数据并评估模型的预测准确度
  - 最好的模型针对测试集所做的预测一定是最准确的
  - 为了使验证（validation）过程行之有效，需要不带偏差的把数据点随机分派到数据集和测试集中
- 交叉验证（cross-validation）：使用同一个数据集进行训练和测试，避免因为原始数据集很小而无法留出足够的数据形成测试集的情况
  - 把数据集划分成若干组用来对模型进行反复测试
  - 在单次迭代中除了某一组外其他各组都被用来训练预测模型，然后留下来的那组被用来测试模型
  - 这个过程重复进行直到每一个组都测试过模型，并且只测试过一次

#### 小结

- 数据科学研究的 4 个主要步骤：
  1. 准备待分析的数据
  2. 根据研究需求挑选合适的算法，为数据建立模型
  3. 对算法的参数进行调优，优化模型
  4. 根据准确度评价模型

### k 均值聚类（k-means clustering）

#### 定义群组

- 群组数量要足够大，以便提取有意义的模式用作商业决策参考，还要足够小，能够确保各个群组之间有明显的区别
- 使用陡坡图（scree plot）确定合适的群组数量
  - 陡坡图可以展现群组内散度随群组数量增加而降低的过程
  - 陡坡图曲线的拐弯处表示最佳群组数量，此处的群组内散度较为合理
- 通过检查群组成员与群组中心点的距离判断该群组的有效性（群组最好由密集的数据点组成）
  1. 猜测每个群组的中心点，因为暂时不能确定通过猜测得到的中心点是否正确，所以称它们为伪中心点
  2. 把每个数据点分配给最近的伪中心点
  3. 根据群组成员的分布调整为中心点的位置
  4. 重复步骤 2 和步骤 3 直至群组成员不再发生变化
- 聚类也可以在更多的维度上进行，虽然多维度分析很难进行可视化，但是可以借助程序计算数据点和群组中心点在多维度情形下的距离

#### 局限性

- 每个数据点只能属于一个群组：恰好位于两个群组中间的数据点无法确定应属于哪个群组
- 群组被假定是正圆形的：若群组的实际形状是椭圆形，那么位于椭圆两端的数据点可能被划入邻近的其他群组
- 群组被假定是离散的：k 均值聚类既不允许群组重叠，也不允许它们相互嵌套
- 弥补局限性的方法：先用 k 均值聚类方法大致了解数据结构，再综合运用其他更高级的方法进行深入分析

#### 小结

- k 均值聚类用于把相似的数据点划入同一个群组，群组数量 k 必须事先指定
- 给数据点分组时，首先把各个数据点分配到距离最近的群组中，然后调整群组中心点的位置，重复这2个步骤直到群组中的成员不再发生变化
- k 均值聚类最适用于正圆形、非重叠的群组

### 主成分分析（principal component analysis）

#### 主成分

- 主成分分析用于找出最能区分数据点的变量，这种变量被称为主成分，数据点会沿着主成分的维度最大限度地分散开
- 主成分可以用已有的一个或多个变量表示
- 标准化（standardization）类似于使用百分位数表示每个变量，以此将所有变量统一到一个标准尺度上
- 采用主成分分析之后，可以不再通过试错法组合变量，而是通过精确计算各个变量的权重来获得最优变量组合

#### 确定主成分数量

- 由于主成分来源于原始变量，因此用来区分数据点的可用信息会受到原始变量个数的制约
- 为了让结果更简单、更通用，应该只选择前几个主成分进行可视化和后续分析
  - 将主成分按照其对数据点的区分效果进行排列
  - 随着主成分个数增多，区分数据点的效果会变差
  - 陡坡图曲线的拐弯处往往体现了最佳主成分数量
- 对当前的数据样本进行解释时，使用的主成分越少，泛化能力就越强

#### 局限性

- 散度最大化：主成分分析有个重要假设，即数据点最分散的维度是最有用的
- 解释成分：主成分分析必须对其产生的成分进行解释，但有时可能很难解释变量按某种方式进行组合的原因
- 正交成分：主成分分析算法成分之间存在正交关系，然而真实信息维度之间可能不存在正交关系
- 弥补局限性的方法：独立成分分析（不需要假设正交关系，在确定成分时还无需考虑数据的散度）

#### 小结

- 主成分分析是一种降维技巧，它使得我们可以使用较小的变量来描述数据，这些变量即为主成分
- 每个主成分都是原始变量的某种加权组合，最好的主成分可以用来改进数据分析和可视化
- 当信息最丰富的几个维度拥有最大的数据散度，并且彼此正交时，主成分分析能有最佳效果

### 关联规则（association rule）

#### 支持度、置信度和提升度

- 识别关联规则的常用指标有3个：支持度、置信度和提升度
- 支持度：某个项集出现的频率，可以人为设定一个支持度阈值，当某个项集的支持度高于这个阈值时，就把它称为频繁项集
- 置信度：当 X 项出现时 Y 项同时出现的频率，记为 {X->Y} ，但它可能会错估某个关联规则的重要性
- 提升度：X 项和 Y 项一同出现的频率，同时考虑这两项各自出现的频率
- {X->Y} 的提升度 = {X->Y} 的置信度 / {Y} 的支持度

#### 先验原则（apriori principal）

- 先验原则是指如果某个项集出现的不频繁那么包含它的任何更大的项集必定也出现的不频繁
- 寻找具有高支持度的项集
  1. 列出只包含一个元素的项集
  2. 计算每个项集的支持度，保留那些满足最小支持度阈值条件的项集，淘汰不满足的项集
  3. 项候选项集中增加一个元素，并利用在步骤2中保留下来的项集产生所有可能的组合
  4. 重复步骤2和步骤3，为越来越大的项集确定支持度，直到没有待检查的新项集
- 寻找具有高置信度或高提升度的关联规则
  - 因为置信度和提升度都是基于支持度计算出来的，因此一旦识别出具有高支持度的项集，寻找关联规则就不会那么费劲了

#### 局限性

- 计算成本高：当库存量很大或者支持度阈值很低时，候选项集仍然会很多
- 假关联：当元素的数量很大时，偶尔会出现假关联，为了确保所发现的关联规则具有普遍性，应该对他们进行验证
- 弥补局限性的方法：使用高级数据结构对候选项集进行更高效的分类，从而减少比较的次数

#### 小结

- 关联规则用于揭示某一元素出现的频率以及它与其他元素的关系
- 识别关联规则的常用指标有3个
  1. {X} 的支持度表示 X 项出现的频率
  2. {X->Y} 的置信度表示当 X 项出现时，Y 项同时出现的频率
  3. {X->Y} 的提升度表示 X 项和 Y 项一同出现的频率，并且考虑每项各自出现的频率
  4. 利用先验原则可以淘汰一大部分非频繁项集，从而大大地加快搜索频繁相机的速度

### 社会网络分析

#### Louvain 方法（Louvain method）

- 通过对节点分组可以找出网络中存在的群组，研究这些群组有助于理解网络各部分的区别和联系
- Louvain 方法用来在网络中找出群组，它会尝试使用不同的聚类配置来做如下两种事：
  1. 同一群组中各个节点间的边数和强度最大化
  2. 把属于不同群组的节点间的边数和强度最小化
- 模块度用于表示上述两件事的完成程度，模块度越高，群组越理想
- 为了获得理想的聚类配置，Louvain 方法会不断迭代
  1. 把每个节点看作一个群组，即一开始群组数和节点数相同
  2. 把一个节点重新分配给对提高模块度有最大帮助的群组，如果无法进一步提高，模块度节点保持不动，针对每个节点重复这个过程直到不能再分配
  3. 把步骤2中发现的每个群组作为一个节点构建出一个粗粒度网络，并且把以前的群间边合并成连接新结点且带权重的边
  4. 重复步骤2和步骤3直到无法再重新分配和合并
- Louvain 方法先发现小群组，然后在适当的情况下合并它们，帮助我们找出更重要的群组，但它有一定的局限性
  - 重要但较小的群组可能会被合并：需要检查在中间迭代阶段被发现的群组，如果有必要就把它们保留下来
  - 有多种可能的聚类配置：若网络中包含重叠或嵌套的群组，需要依据其他信息源对群组予以验证

#### PageRank 算法（PageRank algorithm）

- 虽然群组可以反映出相互作用高度集中的区域，但是这些相互作用可能受占主导地位的节点支配，群组则围绕着这些主导节点形成，为了找出占主导地位的节点，需要对节点进行排序
- PageRank 算法是谷歌公司最初用来为网页排名的算法之一，以 Larry Page 的姓氏命名
- 在PageRank 算法中，决定一个网页排名的因素有如下3个：
  1. 链接数量：被其他网页链接的次数越多，该网页的访问者可能就越多
  2. 链接强度：这些链接被访问的次数越多，该网页的流量就越大
  3. 链接来源：如果被其他有较高排名的网页链接，那么该网页的排名也会升高
- 尽管 PageRank 算法易于使用，但它有偏向于旧节点的局限性：如果一个新网页包含非常棒的内容但一开始访问者人数很少，那么它的排名就比较低
- 可以定期更新 PageRank 值，让新网页随着自身知名度的提高获得提高排名的机会

#### 小结

- 社会网络分析可用于绘制和分析多个实体之间的关系
- Louvain 方法用于在一个网络中找出群组，具体做法是将群组内部的相互作用最大化，同时把群组之间的相互作用最小化，当群组大小相同且相互分离时，该方法的效果最佳
- PageRank 算法根据链接的数量强度以及来源对网络中的节点进行排序，这个算法有助于找出网络中占主导地位的节点，但对链接数不太多的新节点并不友好

### 回归分析（regression analysis）

#### 趋势线

- 趋势线是做预测时常用的工具，他们很容易生成，也很容易理解
- 一般的趋势往往只涉及单个预测变量，这个变量用来产生预测结果，不过通过添加更多预测变量可以改善预测结果
- 回归分析不但可以通过考虑更多预测变量改善预测结果，还可以比较各个预测变量的强弱

#### 梯度下降法（gradient decent）

- 在回归分析中预测变量的权重是主要参数，通过解方程可以求得最优权重
- 梯度下降法先初步猜测合适的权重组合，再通过一个迭代过程，把这些权重应用于每个数据点做预测，然后调整权重以减少整体预测误差
- 这个过程类似于一步步走到山底下，每走一步梯度下降法都要判断从哪个方向下是最陡峭的，然后朝着那个方向重新校准权重，最终到达最低点，这个点的预测误差最小（经过优化的回归趋势线与梯度上的最低点相对应）
- 除了回归之外，梯度下降法也能用来优化其他模型中的参数，比如支持向量机和神经网络
- 梯度下降法的结果可能会受到初始参数值（下山起点）的影响，若起点下方恰好有一个小凹坑，那么梯度下降法可能会将其误认为是最优点
- 为了降低陷入这种凹坑的风险，可以使用随机梯度下降法，每次迭代并不是采用所有的数据点，而是只从其中选取一个来调整参数，引入多变性，有助于算法逃离凹坑

#### 回归系数

- 在为回归预测变量求得最佳权重之后，需要对它们进行解释
- 回归系数：回归预测变量权重，它表示某个预测变量相比于其他预测变量的影响大小
- 预测变量的度量单位不同也会影响对回归系数的解释，因此应该在做回归分析之前先对预测变量的度量单位进行标准化，经过标准化之后预测变量的系数被称为标准化回归系数

#### 相关系数（correlation coefficient）

- 相关系数：当只存在一个预测变量时，该预测变量的标准化回归系数
- 关联方向：相关系数为正表示预测变量和结果变化的方向一致，为负则表示两者变化方向相反
- 关联强度：r 值越接近于 -1 或 1，预测变量的作用就越大，若值为 0 则表示预测变量和结果之间不存在关系
- 因为相关系数表示单个预测变量的绝对强度，所以相比于回归系数，相关系数在对预测变量进行排序时更可靠

#### 局限性

- 对异常值敏感：回归分析平等地对待所有的数据点，只要存在几个有异常值的数据点，就会给趋势线造成很大的影响，因此在做进一步分析之前，可以先使用散点图找出异常值
- 多重共线性问题（multicollinearity）：如果回归模型包含高度相关的预测变量，就会造成相关预测变量权重失真，因此可以使用更高级的技术，如套索回归或岭回归
- 弯曲的趋势：需要对预测变量的值进行转换，或使用支持向量机等其他算法
- 并不说明存在因果关系

#### 小结

- 回归分析用于寻找最佳拟合线（best-fit line），使得尽可能多的数据点位于这条线上或附近
- 趋势线由带权重的组合预测变量得到，这些权重被称为回归系数，表示某个预测变量相对于其他预测变量的影响强度
- 以下情况下，回归分析的效果最好：
  1. 预测变量之间的关系不强
  2. 无异常值
  3. 趋势可以用直线表示

### k 最近邻算法（k-Nearest Neighbors）和异常检测

#### k 最近邻算法

- k 最近邻算法根据周围数据点的类型对某个数据点进行分类（物以类聚，人以群分）
- 在 k 最近邻算法中，参数 k 表示周围数据点的个数，选择 k 值的过程叫做参数调优，它对预测的准确度起着至关重要的作用
- 使用不同的 k 值进行拟合：
  - 如果 k 值太小，数据点只与最近的邻居匹配，并且随机造成所产生的误差也会被放大
  - 如果 k 值太大，数据点会尝试与更远的邻居匹配，其中隐含的模式会被忽略
  - 只有当 k 值恰到好处时，数据点才会参考合适数量的邻居，使得误差相互抵消，有利于揭示数据中隐藏的趋势
- 为实现理想拟合并把误差降到最低，可以使用交叉验证法对参数 k 进行调优
- 对于二分类问题，可以把 k 设置成一个奇数，以避免出现平局的情况
- 除了用来为数据点分类，k 最近邻算法还可以通过合计周围数据点的值来预测连续值
- 通过使用加权平均值，能够进一步改善预测结果，离数据点越近的邻居，其值越能反映该数据点的真实值，因此赋给它的权重应该更大

#### 异常检测

- k 最近邻算法不仅可以用来预测数据点的类别和取值，还可以用来识别异常，比如检测欺诈行为
- 在异常检测的过程中还可能会有新发现，比如发现之前被忽略的预测变量
- 事实上任何能够产生预测模型的算法都可以用来检测异常，比如在回归分析中，如果某个数据点明显偏离最佳拟合线，那么就会被识别为异常点
- 异常数据点既可能因缺失预测变量所致，也可能因预测模型缺少足够的训练数据所致
- 一旦找到异常数据点，就要将它们从数据集中移除，然后再训练预测模型，减少数据中包含的噪声，进而提高模型的准确度

#### 局限性

- 类别不平衡：可以使用加权投票法来取代少数服从多数原则，确保较近数据点类别的权重比较远的更大
- 预测变量过多：在多个维度上识别和处理近邻会导致你计算量大增，需要降维（dimension reduction）

#### 小结

- k 最近邻算法根据周围数据点的类型对某个数据点进行分类
- k 表示用作参考的数据点的个数，可以使用交叉验证法来确定
- 当预测变量数目不多，并且类别大小差别不大时，k 最近邻算法才能产生非常好的效果
- 不准确的分类可能会被标记为潜在异常

### 支持向量机（support vector machine）

#### 勾画最佳分界线

- 支持向量机的主要目标是得到一条能用于分组的最佳分界线，需要注意的是，能用于分组的分界线可能有很多条
- 为了找出最佳分界线，首先需要从一组中找出距离另一组最近的外围数据点，然后在两组的外围数据点之间画出最佳分界线，由于这些外围数据点在寻找最佳分界线的过程中起了支持作用，因此叫做支持向量
- 支持向量机的一个优点是计算速度很快，仅依靠外围数据点就能找到决策边界
- 这种对数据点子集的依赖也有缺点，这是因为决策边界对支持向量的位置比较敏感，选取不同的数据点作为训练数据，相应支持向量的位置也不同
- 支持向量机算法有一个关键特征——缓冲带
  - 缓冲带允许一定数量的训练数据点位于错误的一边，由此得到一条更软的分界线
  - 缓冲带对异常值有更强的耐扰性，因此对新数据有更强的泛化能力
  - 缓冲带通过调整惩罚参数得到，这个参数决定了对分类误差的宽容度，惩罚参数越大，宽容度就越大，缓冲带也就越宽
  - 为了让模型对当前数据和新数据有较高的预测准确度，可以使用交叉验证法求得最佳惩罚参数
- 支持向量机的另一个强项是找到决策边界的凸弧，它在发现错综复杂的凸弧时有着更出众的计算效率
  - 支持向量机的秘诀是核技巧（kernel trick）
  - 支持向量机不会直接在在数据平面上绘制有凸弧的分界线，而是会首先把数据映射到高维空间，然后在高维空间中将数据点用直线分开，这些直线容易计算，并且当映射回低维空间时，也很容易转化成曲线
- 支持向量机具备在高维空间操纵数据的能力，使得它在分析有多个变量的数据集时大受欢迎
- 支持向量机的常见应用场景包括遗传信息破译以及文本情感分析

#### 局限性

- 小数据集：由于支持向量机依靠支持向量确定决策边界，因此样本量少，用来对分界线进行准确定位的数据也少
- 多组数据：支持向量机每次只能对两组进行分类，如果存在2个以上的组，则需要对每一组都应用支持向量机（多类支持向量机）
- 两组之间存在大量重叠：靠近边界的数据点可能更容易发生分类错误，而且支持向量机没有给出每个数据点遭遇错误分类的概率，但可以通过数据点到决策边界的距离来估计其被正确分类的可能性

#### 小结

- 支持向量机用来把数据点分为两组，其方法是在两组的外围数据点（支持向量）的中间画一条分界线
- 支持向量机对异常值有较好的容忍度，它通过一个缓冲带允许少量数据点位于错误的一边，此外他还通过核技巧高效地求得带凸弧的决策边界
- 当需要把大样本中的数据点分为两组时，支持向量机能够发挥最佳作用

### 决策树（decision tree）

#### 生成决策树

- 递归拆分（recursive partitioning）
  1. 确定一个二元选择题，它能够把数据点拆分成两组，并最大限度地提高每组数据点的同质性
  2. 针对每个叶结点重复步骤1，直到满足终止条件
- 终止条件可能有多个，可以使用交叉验证法进行选取
  - 每个叶节点的数据全属于同一类或有相同的值
  - 叶节点包含的数据点少于5个
  - 进一步分支会超出阈值并且不能提高同质性
- 由于递归拆分只用最佳二元选择题来生成决策树，因此不显著的变量并不会影响结果
- 而且二元选择题往往围绕着最重要的值划分数据点，所以决策树对异常值有较强的耐扰性
- 决策树易于可视化，使我们更容易评估预测变量及其相互作用

#### 局限性

- 不稳定：决策树是通过把数据点分组生成的，数据中的细微变化可能影响拆分结果，并导致生成的决策树截然不同
- 容易产生过拟合：决策树每次拆分数据点时都力求找到最佳拆分方式
- 不准确：一开始就是用最佳二元选择题拆分数据点并不能保证结果最准确
- 弥补局限性的方法：每次拆分时可以不采用最佳拆分方式，而是尽量让决策树多样化，然后综合不同的决策树产生的预测结果，让最终预测结果具有更好的稳定性和准确性

#### 决策树的多样化方法

- 随机森林：随机选择不同的二元选择题，生成多棵决策树，然后综合这些决策树的预测结果
- 梯度提升（gradient boosting）：有策略地选择二元选择题，以逐步提高决策树的预测准确度，然后将所有的预测结果的加权平均数作为最终结果

虽然随机森林的和梯度提升能够产生更准确的预测结果，但是它们往往比较复杂并且很难进行可视化（黑盒）

#### 小结

- 决策树通过询问一系列二元选择题来做预测
- 若想生成决策树，就要不断拆分数据样本以获得同质组，直到满足终止条件，这个过程被称为递归拆分
- 虽然决策树易于使用和理解，但是容易造成过拟合问题，导致出现不一致的结果，为了尽量避免出现这种情况，可以采用随机森林等替代方法

### 随机森林

#### 集成模型

- 集成方法（ensembling）：通过组合有不同优缺点的模型来提高预测准确度的方法
- 集成模型：通过组合许多模型的预测结果得到的预测模型，在组合模型时，既可以遵循少数服从多数的原则，也可以取平均值
- 随机森林是决策树的集成模型
- 相比于子模型，集成模型的预测准确度更高，这是因为准确的预测模型会彼此强化，错误的则会彼此抵消
  - 为了达到这种效果，集成模型的子模型一定不能犯同类错误，换言之，子模型必须是不相关的
- 自助聚集法：用来生成不相关的决策树的系统化方法

#### 自助聚集法（bootstrap aggregating）

- 自助聚集法用来生成数千棵决策树，这些树彼此有明显的不同
- 为使决策树之间的关联度最小化，每棵树都由训练数据集的一个随机子集产生，并且使用的是预测变量的一个随机子集，这让生成的决策树各不相同，但仍然保留了一定的预测能力
- 通过限制每次拆分时所用的预测变量，能够生成各不相同的决策树，从而避免发生过拟合问题
- 为进一步降低发生过拟合问题的可能性，可以增加随机森林中决策树的数量，使模型更通用、更准确

#### 局限性

- 随机森林由随机生成的决策树组成，并不存在明确的预测规则，这种不可解释性可能会带来一些伦理问题
- 因此随机森林适用于那些预测准确度比可解释性更重要的场合

#### 小结

- 随机森林的预测结果往往比单棵决策树更准确，这是因为它充分利用了两种技术：自助聚集法和集成方法
- 自助聚集法通过随机限制数据拆分过程所用的变量来生成一系列不相关的决策树
- 集成方法则把决策树的预测结果组合在一起
- 虽然随机森林的预测结果不具有可解释性，但仍然可以根据对预测结果的贡献度大小对各个预测变量进行排序

### 神经网络（neural network）

#### 神经网络的诞生

- 数据存储和共享技术取得进步：为训练神经网络提供了海量数据，有助于改善神经网络的性能
- 计算能力越来越强大：GPU的运行速度最快能达到CPU的150倍，能为在大数据集上训练神经网络提供强大的支持
- 算法获得改进

#### 神经网络的构成

- 输入层：该层处理输入图像的每个像素
  - 为提高预测准确度，可以使用卷积层
  - 卷积层并不处理单个像素，而是识别像素组合的特征，这种分析只关注特征是否出现而不关注出现的位置
  - 所以即使某些关键特征偏离了中心，神经网络仍然能够正确识别，这种特性叫做平移不变性（translational invariance）
- 隐藏层：在像素进入神经网络之后，通过层层转换不断提高和那些标签已知的图像的相似度
  - 标签已知是指神经网络以前见过这些图像
  - 虽然转换得越多，预测准确度就会越高，但是处理时间就会明显增加，一般来说几个隐藏层就足够了
  - 每层的神经元数量要和图像的像素数成比例
- 输出层：该层产生最终预测结果，在这一层中神经元可以只有一个，也可以和结果一样多
- 损失层：该层通常位于最后，并提供有关输入是否识别正确的反馈，如果不正确则给出误差量
  - 在训练神经网络的过程中，损失层至关重要，若预测准确，来自于损失层的反馈会强化产生该预测结果的激活路径
  - 若预测错误，则错误会沿着路径逆向返回，这条路径上的神经元的激活条件就会被重新调整，以减少错误，这个过程称为反向传播（back propagation）
- 通过不断重复这个训练过程，神经网络会学习输入信号和正确输出标签之间的联系，并且把这些联系作为激活规则（activation rule）编入每个神经元，因此为了提高神经网络的预测准确度，需要调整管理激活规则的部件

#### 激活规则

- 为了产生预测结果，需要沿着一条路径依次激活神经元，每个神经元的激活过程都由其激活规则所控制，激活规则指定了输入信号的来源和强度，在神经网络的训练过程中激活规则会不断调整
- 良好的激活规则有助于产生准确的预测结果，其关键在于确定合适的权重和阈值
- 另外神经网络的其他参数也需要调整，比如隐藏层的数量、每层的神经元数量等。可以使用梯度下降法优化这些参数

#### 局限性

- 需要大样本：神经网络的复杂性使之能够识别带有复杂特征的输入，但前提是我们能为他提供大量训练数据，如果训练集太小就会出现过拟合问题
- 降低过拟合风险：
  - 二次取样（subsampling）：为了降低神经元对噪声的敏感度，需要对神经网络的输入进行平滑化处理，即针对信号样本取平均值
  - 畸变：当缺少训练数据时，可以通过向每幅图像引入畸变，来产生更多数据，每幅畸变图像都可以作为新的输入，以此扩大训练数据的规模，畸变应该能够反映原数据集的特征（弹性变形）
  - 丢弃（dropout）：为解决小的神经元集群之间彼此产生过度依赖，可以在训练期间随机丢弃一半的神经元，这些遭丢弃的神经元将处于未激活的状态，剩下的神经元则正常工作，下一次训练丢弃一组不同的神经元，迫使不同的神经元协同工作，从而揭示训练样本所包含的更多特征
- 计算成本高：训练一个由几千个神经元组成的神经网络可能需要很长时间，一个简单的解决方法是升级硬件
- 另一个解决方法是调整算法，用稍低一些的预测准确度换取更快的处理速度
  - 随机梯度下降法：为了更新某一个参数，经典的梯度下降法在一次迭代中使用所有的训练样本，当数据集很大时这样做会耗时，随机梯度下降法是在每次迭代中只用一个训练样本来更新参数，虽然使用这个方法得到的最终参数可能不是最优的，但是准确度不会太低
  - 小批次梯度下降法：虽然使用随机梯度下降法能够提升速度，但最终参数可能不准确，算法也可能无法收敛，导致某个参数上下波动，小批次梯度下降法是每次迭代使用训练样本的一个子集
  - 全连接层：随着加入的神经元越来越多，路径的数量呈指数增长，为避免查看所有可能的组合，可以使初始层（处理更小、更低级的特征）的神经元部分连接，只有最后几层（处理更大、更高级的特征）才对相邻层的神经元进行全连接
- 不可解释：神经网络由多层组成，每层都有几百个神经元，这些神经元有不同的激活规则控制，这使得我们很难准确地找到产生正确预测结果的输入信号组合
  - 但和回归分析不同，回归分析能明确地识别重要的预测变量，并比较它们的强度，神经网络的特性使之难以证明其使用得当，在涉及伦理问题时尤其如此
- 尽管存在上述局限性，但是神经网络本身拥有的强大能力，使之得以应用于虚拟助手、自动驾驶等前沿领域，随着算法不断改进以及计算能力不断提升，神经网络将在物联网时代发挥关键作用

#### 小结

- 神经网络由多个神经元组成，训练期间，第1层的神经元首先被输入数据激活，然后将激活状态传播到后续各层的神经元，最终在输出层产生预测结果
- 一个神经元是否被激活取决于输入信号的来源和强度，这由其激活规则指定，激活规则会根据预测结果的反馈不断调整，这个过程被称为反向传播
- 在大数据集和先进的计算硬件可用的情况下，神经网络的表现最好，然而预测结果在大部分的时候都是无法解释的

### A/B 测试（A/B testing）和多臂老虎机（multi-arm bandit problem）

#### A/B 测试的局限性

- 测试结果具有偶然性：为提高测试结果的可信度可以增加受测人数，但是会导致另一个问题
- 潜在的收入损失：如果把受测顾客增加一倍，那么看到糟糕广告的人数也会增加一倍，这有流失的顾客的风险
- 这两个问题体现了 A/B 测试中的两个权衡因素：探索和利用

#### epsilon 递减策略（epsilon-decreasing strategy）

- epsilon 指的是探索时间与总时间的比例，随着对效果较好的广告越来越有信心，我们使 epsilon 值递减，这个方法属于强化学习的范畴
- A/B 测试由探索和利用前后两个阶段组成，而在 epsilon 递减策略中，探索阶段和利用阶段是分散的，并且一开始时探索得多一些，越接近尾声，探索得越少

#### 多臂老虎机

- 假设有两台老虎机 A 和 B 可供选择，玩2000个回合，每个回合要么赢1美元，要么没有收益，老虎机 A 的返还率为50%，老虎机 B 的则为40%，但我们事先并不知道这些信息，问要怎么玩才能多赢钱？
  - 全探索：如果随机选择老虎机，平均会赢900美元
  - A/B 测试：用前200个回合探索哪台老虎机的返还率更高，然后在剩下的1800个回合中选择这台老虎机，平均会赢976美元，但由于两台老虎机的返还率接近，由此存在误判的可能性（误判概率8%）
  - 为降低误判的风险，可以把A/B 测试的探索范围扩大到500个回合，这样做可以把误判概率降到1%，但是平均中奖金额也会减少到963美元
  - epsilon 递减策略：如果采用 epsilon 递减策略边探索边利用，平均会赢984美元，并且误判概率为4%，通过增加探索比例（增加 epsilon 值）能够降低误判概率，但仍会减少平均中奖金额
  - 全利用：如果一开始就掌握内部消息并选择返还率最高的老虎机 A，平均会赢1000美元，但这个假设不现实
- 由此看到，在不掌握内部消息的情况下，采用 epsilon 递减策略的收益最高，而且由于存在收敛性这一数学特征，由此 epsilon 递减策略能确保在回合数足够多的情况下找出返还率最高的老虎机
- 胜者为先：范加尔在曼联采用了一个非常规策略来决定罚点球的球员，第一个被指定罚点球的球员会负责到底，除非他没有打进球，接下来新换的球员继续负责罚点球，如果没有罚进就再换一名球员，依此类推
  - 频繁地换老虎机，会导致探索过多而利用过少，只比随机选择老虎机稍好一些，而且胜者为先策略只根据上一次的结果来评估老虎机，这忽略了老虎机之前的表现

#### epsilon 递减策略的局限性

- 采用 epsilon 递减策略的关键在于控制好 epsilon 值，如果 epsilon 值递减的过慢，就会失去利用老虎机的机会，而如果递减得过快，就可能会选错老虎机
- epsilon 值的最佳递减速度主要取决于两台老虎机返还率的相似程度，采用汤普森取样方法，可以计算 epsilon 值
- epsilon 递减策略还依赖于如下假设：
  - 返还率恒定不变
  - 返还率与上一次游戏无关
  - 玩游戏和观察返还率之间的延迟极小

#### 小结

- 多臂老虎机问题的实质是如何以最佳方式分配资源
- 一种策略是先探索可用选项，然后把所有剩余资源分配给表现最佳的选项，这个策略叫做 A/B 测试
- 另一个策略是给表现最佳的选项，逐渐分配更多的资源，这个策略叫做 epsilon 递减策略
- 虽然 epsilon 递减策略在大多数情况下能够提供比 A/B 测试更高的回报，但是确定资源分配的最佳更新速度并非易事

### 更多评价指标

#### 分类指标

- 接受者操作特征曲线下面积（曲线下面积）：这个指标允许我们在最大化正例率和最小化假正例率之间做权衡
  - 正例率：被模型正确预测为正类别的样本所占的比例
  - 正例率 = 正例数 / ( 正例数 + 假负例数 )
  - 假正例率：被模型错误预测为正类别的样本所占的比例
  - 假正例率 = 假正例数 / ( 假正例数 + 负例数 )
  - 在极端情况下，可以把所有样本全部预测为正类别，以此实现正例率最大化（正例率为1），虽然这样做可以避免出现假负例但会明显增加假正例
  - 接受者操作特征曲线（ROC 曲线）体现了最大化正例率和最小化假正例率之间的权衡
  - 模型性能通过 ROC 曲线下方的面积来衡量，所以该指标被称为曲线下面积，模型的准确度越高，曲线越靠近左上角
  - 完美的预测模型会产生一条曲线下面积为1的曲线（曲线下面积等于整个图形的面积）
- 对数损失指标利用置信度来校正其对预测误差的惩罚，具体来说，模型对错误预测的置信度越高，惩罚就越重
  - 由于对数损失指标根据对预测结果的置信度来调整惩罚程度，因此它通常用于错误预测极其有害的情况

#### 回归指标

- 平均绝对误差：平等的惩罚所有预测误差，具体做法是对所有数据点的预测值和实际值之差取平均值
- 除了考虑误差大小之外，还可以通过均方根对数误差把误差方向纳入考虑范围
