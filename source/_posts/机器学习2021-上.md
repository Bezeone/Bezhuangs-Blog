---
title: 机器学习（上）
date: 2021-09-23
updated: 2021-09-25
tags: [Machine Learning]
group: todo
mathjax: true
categories: 计算机专业
references:
  - title: 李宏毅机器学习2017笔记
    url: https://datawhalechina.github.io/leeml-notes/#/
---

> 机器学习是人工智能的重要技术基础，涉及的内容十分广泛，涵盖概率论知识，统计学知识，近似理论知识和复杂算法知识，使用计算机作为工具并致力于真实实时的模拟人类学习方式。我选择的课程是台湾大学电机工程系助理教授李宏毅的 [ML 2021 Spring](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html)，这门课程从最基本的观念讲到最前瞻的技术，重点讲解深度学习，同时新增了 Transformer、流模型 Glow 和对抗攻击等最新技术内容。以下为所记课堂笔记第一部分，包含机器学习基本概念理论、深度学习和自注意力机制，可供参考。其他部分笔记详见[机器学习（中）](/机器学习2021-中)和[机器学习（下）](/机器学习2021-下)

<!--more-->

{% image https://cdn.jsdelivr.net/gh/Bezhuang/Imgbed/blogimg/李宏毅机器学习01.jpg, width=500px %}

### 机器学习介绍

#### 机器学习基本概念

- Machine Learning ≈ Looking for Function（寻找复杂函数的过程）
- 机器学习的分类
  - Regression（回归）: The function outputs a scalar
  - Classification（分类）: Given options (classes), the function outputs the correct one
  - Structured Learning（结构化学习）: create something with structure (image, document)

- Regression（回归）的 Training 过程

  1. Function with Unknown Parameters

     - Based on domain knowledge

     - Model: $y=b+w x_1$
     - $w$ (weight) and $b$ (bias) are unknown parameters learned from data（模型参数）

  2. Define Loss from Training Data

     - Loss is a function of parameters: $L(b,w)$
     - Loss用来评估未知参数准确度 (how good a set of values is): $L=\dfrac{1}{N}\sum _{n}e_{n}$
     - MAE（绝对平均误差）: 𝐿 is mean absolute error: $e=\left|y-\widehat{y}\right|$
     - MSE（均方误差）: 𝐿 is mean square error: $e=\left( y-\widehat{y}\right)^{2}$
     - If $𝑦$ and $\widehat{y}$ are both probability distributions: 选择交叉熵（Cross-entropy）作为 Loss 函数

  3. Optimization（优化）: $w^{\ast },b^{\ast }=\arg \min _{w,b}L$

     - 最常用的方法：梯度下降（Gradient Descent）
     - 首先随机初始化参数：(Randomly) Pick initial values $w^0,b^0$,
     - 再计算该点的 Loss 关于参数的微分值：Compute $\dfrac{\partial L}{\partial w}| w=w^{0},b=b^{0}$
     - 当微分为负数时，Loss 关于 w 递减，增大 w 使 Loss下降。当微分为正数时，Loss 关于 w 递增，减小 w 使 Loss 下降
     - 随之用微分值去更新参数：Update $𝑤,b$ iteratively 
     - 跨越的大小通过学习率控制，方向通过符号实现：$w_{0}\rightarrow w_{1},b_{0}\rightarrow b_{1}$: $\eta \dfrac{2L}{\partial w}| w=w^{0},b=b^{0}$
     - 学习率（Learning rate）是一种超参数（hyperparameter），记做：$\eta$

- Linear models are too simple and have severe limitations, we need more sophisticated and flexible models

#### 深度学习基本概念





### 深度学习

#### 机器学习任务攻略

#### 类神经网络训练不起来怎么办（一）？局部最小值与鞍点

#### 类神经网络训练不起来怎么办（二）？批次与动量

#### 类神经网络训练不起来怎么办（三）？自动调整学习速率

#### 类神经网络训练不起来怎么办（四）？损失函数也可能有影响



### 自注意力

#### 卷积神经网络

#### 自注意力机制（上）

#### 自注意力机制（下）



### 机器学习理论

#### Theory of ML

