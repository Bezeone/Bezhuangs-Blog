---
title: æœºå™¨å­¦ä¹ ï¼ˆä¸Šï¼‰
date: 2021-09-23
updated: 2021-09-25
tags: [Machine Learning]
group: todo
mathjax: true
categories: è®¡ç®—æœºä¸“ä¸š
references:
  - title: æå®æ¯…æœºå™¨å­¦ä¹ 2017ç¬”è®°
    url: https://datawhalechina.github.io/leeml-notes/#/
---

> æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦æŠ€æœ¯åŸºç¡€ï¼Œæ¶‰åŠçš„å†…å®¹ååˆ†å¹¿æ³›ï¼Œæ¶µç›–æ¦‚ç‡è®ºçŸ¥è¯†ï¼Œç»Ÿè®¡å­¦çŸ¥è¯†ï¼Œè¿‘ä¼¼ç†è®ºçŸ¥è¯†å’Œå¤æ‚ç®—æ³•çŸ¥è¯†ï¼Œä½¿ç”¨è®¡ç®—æœºä½œä¸ºå·¥å…·å¹¶è‡´åŠ›äºçœŸå®å®æ—¶çš„æ¨¡æ‹Ÿäººç±»å­¦ä¹ æ–¹å¼ã€‚æˆ‘é€‰æ‹©çš„è¯¾ç¨‹æ˜¯å°æ¹¾å¤§å­¦ç”µæœºå·¥ç¨‹ç³»åŠ©ç†æ•™æˆæå®æ¯…çš„ [ML 2021 Spring](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.html)ï¼Œè¿™é—¨è¯¾ç¨‹ä»æœ€åŸºæœ¬çš„è§‚å¿µè®²åˆ°æœ€å‰ç»çš„æŠ€æœ¯ï¼Œé‡ç‚¹è®²è§£æ·±åº¦å­¦ä¹ ï¼ŒåŒæ—¶æ–°å¢äº† Transformerã€æµæ¨¡å‹ Glow å’Œå¯¹æŠ—æ”»å‡»ç­‰æœ€æ–°æŠ€æœ¯å†…å®¹ã€‚ä»¥ä¸‹ä¸ºæ‰€è®°è¯¾å ‚ç¬”è®°ç¬¬ä¸€éƒ¨åˆ†ï¼ŒåŒ…å«æœºå™¨å­¦ä¹ åŸºæœ¬æ¦‚å¿µç†è®ºã€æ·±åº¦å­¦ä¹ å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä¾›å‚è€ƒã€‚å…¶ä»–éƒ¨åˆ†ç¬”è®°è¯¦è§[æœºå™¨å­¦ä¹ ï¼ˆä¸­ï¼‰](/æœºå™¨å­¦ä¹ 2021-ä¸­)å’Œ[æœºå™¨å­¦ä¹ ï¼ˆä¸‹ï¼‰](/æœºå™¨å­¦ä¹ 2021-ä¸‹)

<!--more-->

{% image https://cdn.jsdelivr.net/gh/Bezhuang/Imgbed/blogimg/æå®æ¯…æœºå™¨å­¦ä¹ 01.jpg, width=500px %}

### æœºå™¨å­¦ä¹ ä»‹ç»

#### æœºå™¨å­¦ä¹ åŸºæœ¬æ¦‚å¿µ

- Machine Learning â‰ˆ Looking for Functionï¼ˆå¯»æ‰¾å¤æ‚å‡½æ•°çš„è¿‡ç¨‹ï¼‰
- æœºå™¨å­¦ä¹ çš„åˆ†ç±»
  - Regressionï¼ˆå›å½’ï¼‰: The function outputs a scalar
  - Classificationï¼ˆåˆ†ç±»ï¼‰: Given options (classes), the function outputs the correct one
  - Structured Learningï¼ˆç»“æ„åŒ–å­¦ä¹ ï¼‰: create something with structure (image, document)

- Regressionï¼ˆå›å½’ï¼‰çš„ Training è¿‡ç¨‹

  1. Function with Unknown Parameters

     - Based on domain knowledge

     - Model: $y=b+w x_1$
     - $w$ (weight) and $b$ (bias) are unknown parameters learned from dataï¼ˆæ¨¡å‹å‚æ•°ï¼‰

  2. Define Loss from Training Data

     - Loss is a function of parameters: $L(b,w)$
     - Lossç”¨æ¥è¯„ä¼°æœªçŸ¥å‚æ•°å‡†ç¡®åº¦ (how good a set of values is): $L=\dfrac{1}{N}\sum _{n}e_{n}$
     - MAEï¼ˆç»å¯¹å¹³å‡è¯¯å·®ï¼‰: ğ¿ is mean absolute error: $e=\left|y-\widehat{y}\right|$
     - MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰: ğ¿ is mean square error: $e=\left( y-\widehat{y}\right)^{2}$
     - If $ğ‘¦$ and $\widehat{y}$ are both probability distributions: é€‰æ‹©äº¤å‰ç†µï¼ˆCross-entropyï¼‰ä½œä¸º Loss å‡½æ•°

  3. Optimizationï¼ˆä¼˜åŒ–ï¼‰: $w^{\ast },b^{\ast }=\arg \min _{w,b}L$

     - æœ€å¸¸ç”¨çš„æ–¹æ³•ï¼šæ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰
     - é¦–å…ˆéšæœºåˆå§‹åŒ–å‚æ•°ï¼š(Randomly) Pick initial values $w^0,b^0$,
     - å†è®¡ç®—è¯¥ç‚¹çš„ Loss å…³äºå‚æ•°çš„å¾®åˆ†å€¼ï¼šCompute $\dfrac{\partial L}{\partial w}| w=w^{0},b=b^{0}$
     - å½“å¾®åˆ†ä¸ºè´Ÿæ•°æ—¶ï¼ŒLoss å…³äº w é€’å‡ï¼Œå¢å¤§ w ä½¿ Lossä¸‹é™ã€‚å½“å¾®åˆ†ä¸ºæ­£æ•°æ—¶ï¼ŒLoss å…³äº w é€’å¢ï¼Œå‡å° w ä½¿ Loss ä¸‹é™
     - éšä¹‹ç”¨å¾®åˆ†å€¼å»æ›´æ–°å‚æ•°ï¼šUpdate $ğ‘¤,b$ iteratively 
     - è·¨è¶Šçš„å¤§å°é€šè¿‡å­¦ä¹ ç‡æ§åˆ¶ï¼Œæ–¹å‘é€šè¿‡ç¬¦å·å®ç°ï¼š$w_{0}\rightarrow w_{1},b_{0}\rightarrow b_{1}$: $\eta \dfrac{2L}{\partial w}| w=w^{0},b=b^{0}$
     - å­¦ä¹ ç‡ï¼ˆLearning rateï¼‰æ˜¯ä¸€ç§è¶…å‚æ•°ï¼ˆhyperparameterï¼‰ï¼Œè®°åšï¼š$\eta$

- Linear models are too simple and have severe limitations, we need more sophisticated and flexible models

#### æ·±åº¦å­¦ä¹ åŸºæœ¬æ¦‚å¿µ





### æ·±åº¦å­¦ä¹ 

#### æœºå™¨å­¦ä¹ ä»»åŠ¡æ”»ç•¥

#### ç±»ç¥ç»ç½‘ç»œè®­ç»ƒä¸èµ·æ¥æ€ä¹ˆåŠï¼ˆä¸€ï¼‰ï¼Ÿå±€éƒ¨æœ€å°å€¼ä¸éç‚¹

#### ç±»ç¥ç»ç½‘ç»œè®­ç»ƒä¸èµ·æ¥æ€ä¹ˆåŠï¼ˆäºŒï¼‰ï¼Ÿæ‰¹æ¬¡ä¸åŠ¨é‡

#### ç±»ç¥ç»ç½‘ç»œè®­ç»ƒä¸èµ·æ¥æ€ä¹ˆåŠï¼ˆä¸‰ï¼‰ï¼Ÿè‡ªåŠ¨è°ƒæ•´å­¦ä¹ é€Ÿç‡

#### ç±»ç¥ç»ç½‘ç»œè®­ç»ƒä¸èµ·æ¥æ€ä¹ˆåŠï¼ˆå››ï¼‰ï¼ŸæŸå¤±å‡½æ•°ä¹Ÿå¯èƒ½æœ‰å½±å“



### è‡ªæ³¨æ„åŠ›

#### å·ç§¯ç¥ç»ç½‘ç»œ

#### è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¸Šï¼‰

#### è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¸‹ï¼‰



### æœºå™¨å­¦ä¹ ç†è®º

#### Theory of ML

